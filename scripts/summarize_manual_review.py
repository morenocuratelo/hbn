#!/usr/bin/env python3
"""Summarize `docs_qid_manual_review.csv` into a compact markdown report.

Produces: docs/docs_qid_manual_review_summary.md
"""
from pathlib import Path
import csv
from collections import defaultdict

ROOT = Path(__file__).resolve().parent.parent
DOCS = ROOT / 'docs'
MANUAL = DOCS / 'docs_qid_manual_review.csv'
OUT = DOCS / 'docs_qid_manual_review_summary.md'

rows = []
if MANUAL.exists():
    with MANUAL.open(encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for r in reader:
            try:
                r['score_f'] = float(r.get('score') or 0)
            except Exception:
                r['score_f'] = 0.0
            rows.append(r)

by_source = defaultdict(list)
for r in rows:
    by_source[r.get('source_type') or 'unknown'].append(r)

lines = []
lines.append('# Prioritized Manual Review — Low-Confidence Mappings')
lines.append('Generated by `scripts/summarize_manual_review.py` — lists top problem candidates and quick recommended actions.')
lines.append('')
total = len(rows)
lines.append(f'- Total low-confidence rows: **{total}**')
lines.append('')

# Top 15 worst-scoring rows overall
lines.append('## Top 15 Lowest-Confidence Candidates (All Sources)')
lines.append('')
rows_sorted = sorted(rows, key=lambda r: r['score_f'])[:15]
lines.append('| Rank | Source Type | Source | Candidate ID | Score | Recommendation |')
lines.append('|---:|---|---|---|---:|---|')
for i, r in enumerate(rows_sorted, start=1):
    src = r.get('source') or ''
    cid = r.get('candidate_id') or ''
    score = f"{r.get('score_f'):.3f}"
    # quick recommendation
    if cid:
        rec = 'Verify candidate; if correct, accept mapping.'
    else:
        rec = 'No candidate — inspect context/snippet and assign canonical ID manually.'
    lines.append(f'| {i} | {r.get("source_type")} | {src} | {cid} | {score} | {rec} |')

lines.append('')

# Per-source summary (top 5 each)
lines.append('## Per-Source Top Issues (Top 5 by lowest score)')
lines.append('')
for stype, items in sorted(by_source.items(), key=lambda x: (-len(x[1]), x[0])):
    lines.append(f'### {stype} — {len(items)} rows')
    lines.append('')
    sub = sorted(items, key=lambda r: r['score_f'])[:5]
    lines.append('| Source | Candidate ID | Score | Context Snippet | Quick Action |')
    lines.append('|---|---|---:|---|---|')
    for r in sub:
        src = r.get('source') or ''
        cid = r.get('candidate_id') or ''
        score = f"{r.get('score_f'):.3f}"
        snippet = (r.get('context_snippet') or '').replace('\n',' ')[:120]
        if cid:
            action = 'Verify candidate; accept or correct.'
        else:
            action = 'Assign candidate manually (use canonical list in `data.js`).'
        lines.append(f'| {src} | {cid} | {score} | {snippet} | {action} |')
    lines.append('')

# Short guidance
lines.append('## Quick Review Guidance')
lines.append('')
lines.append('- Prefer accepting mappings with score >= 0.6 after manual eyeballing.')
lines.append('- If context snippet contains clear topic words (e.g., "dopamine", "Maguire", "Google Effect"), search `data.js` for matching canonical question texts.')
lines.append('- For RTF QIDs (source_type `rtf_qid`), prioritize mapping those that appear in `docs_qid_expanded.csv` and cross-check with `docs_qid_map_expanded.csv`.')
lines.append('- Record any manual corrections in `docs/docs_qid_map_final.csv` or open a PR with changes.')

OUT.write_text('\n'.join(lines), encoding='utf-8')
print(f'Wrote summary: {OUT}')
